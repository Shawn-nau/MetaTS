{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os, sys, gc, time, warnings, pickle, random\n",
    "\n",
    "from tsfmeta.data import MetaDataset,RawData, Md_utils,temporal_signal_split\n",
    "from tsfmeta import utils\n",
    "from tsfmeta import nn as MetaNN\n",
    "from tsfmeta.experiments import meta_learning_run\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "dir_ = 'C:/M5/' # input only here\n",
    "raw_data_dir = dir_+'M5-point/' \n",
    "processed_data_dir = 'E:/temp/'\n",
    "TARGET = 'sales'         # Our main target\n",
    "END_TRAIN = 1941-28         # Last day in train set\n",
    "MAIN_INDEX = ['id','d']  # We can identify item by these columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_df = pd.read_csv(raw_data_dir+'sales_train_validation.csv')\n",
    "prices_df = pd.read_csv(raw_data_dir+'sell_prices.csv')\n",
    "calendar_df = pd.read_csv(raw_data_dir+'calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_columns = ['id','item_id','dept_id','cat_id','store_id','state_id']\n",
    "sale_df = pd.melt(sale_df, \n",
    "                  id_vars = index_columns, \n",
    "                  var_name = 'd', \n",
    "                  value_name = TARGET)\n",
    "for col in index_columns:\n",
    "    sale_df[col] = sale_df[col].astype('category')\n",
    "\n",
    "icols = ['d', 'wm_yr_wk', 'wday', 'month',  'event_type_1', 'snap_CA',  'snap_TX', 'snap_WI',]\n",
    "\n",
    "sale_df = sale_df.merge(calendar_df[icols], on=['d'], how='left')\n",
    "sale_df = sale_df.merge(prices_df, on=['store_id','item_id','wm_yr_wk'], how='left')\n",
    "sale_df =  sale_df.drop(['wm_yr_wk','item_id'], axis=1)\n",
    "sale_df['sell_price'] =  np.log(sale_df['sell_price'])\n",
    "sale_df['d'] = sale_df['d'].apply(lambda x: x[2:]).astype(np.int16)  # Convert 'd' to int\n",
    "sale_df.drop(sale_df.loc[sale_df['sell_price'].isna()].index, inplace=True)\n",
    "\n",
    "le = LabelEncoder()\n",
    "for col in ['event_type_1', 'snap_CA',  'snap_TX', 'snap_WI','dept_id','cat_id','store_id','state_id']:\n",
    "    sale_df[col] = le.fit_transform(sale_df[col].values)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HOBBIES_1_008_CA_1_validation</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.776529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>HOBBIES_1_009_CA_1_validation</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.444686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HOBBIES_1_010_CA_1_validation</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.153732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HOBBIES_1_012_CA_1_validation</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.788421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>HOBBIES_1_015_CA_1_validation</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.356675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58327365</th>\n",
       "      <td>FOODS_3_823_WI_3_validation</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1913</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.091923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58327366</th>\n",
       "      <td>FOODS_3_824_WI_3_validation</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1913</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.908259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58327367</th>\n",
       "      <td>FOODS_3_825_WI_3_validation</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1913</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.381282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58327368</th>\n",
       "      <td>FOODS_3_826_WI_3_validation</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1913</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.246860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58327369</th>\n",
       "      <td>FOODS_3_827_WI_3_validation</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1913</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>46027957 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  dept_id  cat_id  store_id  state_id  \\\n",
       "7         HOBBIES_1_008_CA_1_validation        3       1         0         0   \n",
       "8         HOBBIES_1_009_CA_1_validation        3       1         0         0   \n",
       "9         HOBBIES_1_010_CA_1_validation        3       1         0         0   \n",
       "11        HOBBIES_1_012_CA_1_validation        3       1         0         0   \n",
       "14        HOBBIES_1_015_CA_1_validation        3       1         0         0   \n",
       "...                                 ...      ...     ...       ...       ...   \n",
       "58327365    FOODS_3_823_WI_3_validation        2       0         9         2   \n",
       "58327366    FOODS_3_824_WI_3_validation        2       0         9         2   \n",
       "58327367    FOODS_3_825_WI_3_validation        2       0         9         2   \n",
       "58327368    FOODS_3_826_WI_3_validation        2       0         9         2   \n",
       "58327369    FOODS_3_827_WI_3_validation        2       0         9         2   \n",
       "\n",
       "             d  sales  wday  month  event_type_1  snap_CA  snap_TX  snap_WI  \\\n",
       "7            1     12     1      1             4        0        0        0   \n",
       "8            1      2     1      1             4        0        0        0   \n",
       "9            1      0     1      1             4        0        0        0   \n",
       "11           1      0     1      1             4        0        0        0   \n",
       "14           1      4     1      1             4        0        0        0   \n",
       "...        ...    ...   ...    ...           ...      ...      ...      ...   \n",
       "58327365  1913      1     2      4             4        0        0        0   \n",
       "58327366  1913      0     2      4             4        0        0        0   \n",
       "58327367  1913      0     2      4             4        0        0        0   \n",
       "58327368  1913      3     2      4             4        0        0        0   \n",
       "58327369  1913      0     2      4             4        0        0        0   \n",
       "\n",
       "          sell_price  \n",
       "7          -0.776529  \n",
       "8           0.444686  \n",
       "9           1.153732  \n",
       "11          1.788421  \n",
       "14         -0.356675  \n",
       "...              ...  \n",
       "58327365    1.091923  \n",
       "58327366    0.908259  \n",
       "58327367    1.381282  \n",
       "58327368    0.246860  \n",
       "58327369    0.000000  \n",
       "\n",
       "[46027957 rows x 14 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['sales']\n",
    "features_num = ['sell_price'] \n",
    "idx = ['id']  \n",
    "tdx = ['d']\n",
    "features_cat = ['event_type_1', 'snap_CA',  'snap_TX', 'snap_WI',\n",
    "            'dept_id','cat_id','store_id','state_id', 'wday','month',] \n",
    "dynamic_known_features_num = ['sell_price']  ## should be part of features_num\n",
    "dynamic_known_features_cat = []   ## should be part of features_cat\n",
    "\n",
    "static_known_features = ['dept_id','cat_id','store_id','state_id']\n",
    "\n",
    "df = Md_utils.Df_to_rawdata(df = sale_df,\n",
    "                   idx = idx,\n",
    "                   tdx = tdx,\n",
    "                   target = target,\n",
    "                   features_num = features_num,\n",
    "                   features_cat= features_cat,\n",
    "                   static_known_features = static_known_features,\n",
    "                   dynamic_known_features_num = dynamic_known_features_num,\n",
    "                   dynamic_known_features_cat = dynamic_known_features_cat,\n",
    "                   )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pickle.load(open(raw_data_dir +'M5_meta_rawdata' +'.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216,\n",
       "            ...\n",
       "            1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913],\n",
       "           dtype='int64', name='d', length=707)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_slots = df.data_segment(700,7,5)\n",
    "df_slots[3].npdata.shape\n",
    "df_slots[0].time_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_slots)):\n",
    "    df_slots[i] = df_slots[i].Raw2Meta(H=7)\n",
    "    #df_slots[i].reduce_mem_usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SES example\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from random import random\n",
    "\n",
    "for slot in range(len(df_slots)):\n",
    "\n",
    "    Nts,_, T = df_slots[slot].X.shape \n",
    "    _, H = df_slots[slot].Y.shape\n",
    "    model = [SimpleExpSmoothing(df_slots[slot].X[i,0]).fit() for i in range(Nts)]\n",
    "    model_pred = [model[i].predict(T,T + H -1) for i in range(Nts)]\n",
    "    model_pred = np.stack(model_pred,0)\n",
    "    df_slots[slot].add_Base_forecasts(model_pred,'SimpleExpSmoothing')\n",
    "\n",
    "pickle.dump(df_slots, open(processed_data_dir +'M5_metadata_slots_book' +'.pkl', 'wb'), protocol= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HEWS example\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from random import random\n",
    "\n",
    "for slot in range(len(df_slots)):\n",
    "\n",
    "    Nts,_, T = df_slots[slot].X.shape \n",
    "    _, H = df_slots[slot].Y.shape\n",
    "    model = [ExponentialSmoothing(df_slots[slot].X[i,0], trend = 'add',damped_trend=True).fit() for i in range(Nts)]\n",
    "    model_pred = [model[i].predict(T,T + H -1) for i in range(Nts)]\n",
    "    model_pred = np.stack(model_pred,0)\n",
    "    df_slots[slot].add_Base_forecasts(model_pred,'ExponentialSmoothing')\n",
    "pickle.dump(df_slots, open(processed_data_dir +'M5_metadata_slots_book' +'.pkl', 'wb'), protocol= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb direct example\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for slot in range(len(df_slots)):\n",
    "    print(slot)\n",
    "    gdf = df_slots[slot].XZ_globalReg(L=7, rolling_step = 7, mode= 'Tree')\n",
    "\n",
    "    H = gdf['Y_tr'].shape[-1]\n",
    "    X_train, X_val, y_train, y_val, mask_train, mask_val,  =  train_test_split(np.concatenate([gdf['X_tr'],gdf['Z_tr']],-1)  ,gdf['Y_tr'] ,gdf['mask_tr'], test_size = 0.2, random_state =0)\n",
    "\n",
    "    predicts = []\n",
    "    for h in range(H): \n",
    "        dtrain = lgb.Dataset(X_train , label= y_train[:,h] ,weight = mask_train[:,h] )\n",
    "        dval = lgb.Dataset(X_val , label= y_val[:,h] , weight = mask_val[:,h] )\n",
    "        params = {\n",
    "                'num_leaves': 1024,\n",
    "                'objective': 'regression',\n",
    "                'min_data_in_leaf': 50,\n",
    "                'learning_rate': 0.02,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.7,\n",
    "                'bagging_freq': 1,\n",
    "                'metric': 'rmse',\n",
    "                'num_threads': 8\n",
    "            }\n",
    "            \n",
    "            \n",
    "        MAX_ROUNDS = 2000\n",
    "        bst = lgb.train(\n",
    "                params, dtrain, num_boost_round=MAX_ROUNDS, \n",
    "                        valid_sets=[dtrain,dval], early_stopping_rounds=125, verbose_eval=30\n",
    "                    )\n",
    "\n",
    "        predicts.append( bst.predict(np.concatenate([gdf['X_ts'],gdf['Z_ts']],-1)  , num_iteration=bst.best_iteration or MAX_ROUNDS)[:,np.newaxis])\n",
    "\n",
    "    predicts = np.concatenate(predicts,-1)\n",
    "    df_slots[slot].add_Base_forecasts(predicts,'lightgbm_direct')\n",
    "pickle.dump(df_slots, open(processed_data_dir +'M5_metadata_slots_book' +'.pkl', 'wb'), protocol= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lgb mimo example\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for slot in range(len(df_slots)):\n",
    "\n",
    "    gdf = df_slots[slot].XZ_globalReg(L=14, rolling_step = 7, mode= 'Tree')\n",
    "    H = gdf['Y_tr'].shape[-1]\n",
    "    X = gdf['X_tr'].repeat(H,0)\n",
    "    Y =  gdf['Y_tr'].reshape(-1)\n",
    "    Z = np.stack([gdf['Z_tr'][:,(i*H):(i+1)*H].reshape(-1) for i in range(gdf['Z_tr'].shape[1]//H)],1)\n",
    "    mask = gdf['mask_tr'].reshape(-1)\n",
    "\n",
    "    \n",
    "    X_train, X_val, y_train, y_val, mask_train, mask_val,  =  train_test_split(np.concatenate([X,Z],-1) ,Y ,mask, test_size = 0.2, random_state =0)\n",
    "\n",
    " \n",
    "    X_test = gdf['X_ts'].repeat(H,0)\n",
    "    Z_test = np.stack([gdf['Z_ts'][:,(i*H):(i+1)*H].reshape(-1) for i in range(gdf['Z_ts'].shape[1]//H)],1)\n",
    "    mask_val = mask_val.reshape(-1)\n",
    "    \n",
    "    dtrain = lgb.Dataset(X_train , label= y_train ,weight = mask_train )\n",
    "    dval = lgb.Dataset(X_val , label= y_val, weight = mask_val )\n",
    "    \n",
    "    params = {\n",
    "            'num_leaves': 1024,\n",
    "            'objective': 'regression',\n",
    "            'min_data_in_leaf': 50,\n",
    "            'learning_rate': 0.02,\n",
    "            'feature_fraction': 0.8,\n",
    "            'bagging_fraction': 0.7,\n",
    "            'bagging_freq': 1,\n",
    "            'metric': 'rmse',\n",
    "            'num_threads': 8\n",
    "        }\n",
    "        \n",
    "        \n",
    "    MAX_ROUNDS = 2000\n",
    "    bst = lgb.train(\n",
    "            params, dtrain, num_boost_round=MAX_ROUNDS, \n",
    "                    valid_sets=[dtrain,dval], early_stopping_rounds=125, verbose_eval=30\n",
    "                )\n",
    "\n",
    "    predicts = bst.predict( np.concatenate([X_test,Z_test],-1), num_iteration=bst.best_iteration or MAX_ROUNDS)\n",
    "\n",
    "    predicts = predicts.reshape(len(gdf['X_ts']),H )\n",
    "    df_slots[slot].add_Base_forecasts(predicts,'lightgbm_mimo')\n",
    "pickle.dump(df_slots, open(processed_data_dir +'M5_metadata_slots_book' +'.pkl', 'wb'), protocol= 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SimpleExpSmoothing',\n",
       " 'ExponentialSmoothing',\n",
       " 'lightgbm_direct',\n",
       " 'lightgbm_mimo']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_slots[1].forecasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest example// too slow\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "for slot in range(len(df_slots)):\n",
    "    print(slot)\n",
    "    gdf = df_slots[slot].XZ_globalReg(L=7, rolling_step = 7, mode= 'Tree')\n",
    "    H = gdf['Y_tr'].shape[-1]\n",
    "        \n",
    "    regr = RandomForestRegressor(n_estimators=200 ,max_depth=10, random_state=0, n_jobs = 4 )\n",
    "    regr.fit(X = np.nan_to_num(np.concatenate([gdf['X_tr'],gdf['Z_tr']],-1),nan = -1),\n",
    "             y = np.nan_to_num(gdf['Y_tr'] ,nan = -1),\n",
    "             sample_weight = gdf['mask_tr'][:,0])    \n",
    "    predicts = regr.predict(X = np.nan_to_num(np.concatenate([gdf['X_ts'],gdf['Z_ts']],-1),nan = -1))   \n",
    "    df_slots[slot].add_Base_forecasts(predicts,'rf_mimo')\n",
    "pickle.dump(df_slots, open(processed_data_dir +'M5_metadata_slots_book' +'.pkl', 'wb'), protocol= 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "### meta learning\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slots = pickle.load( open(processed_data_dir +'M5_metadata_slots_book' +'.pkl', 'rb') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-5\n",
    "batch_size = 256\n",
    "\n",
    "meta_train = Md_utils.md_concat([df_slots[i] for i in range(1,5)],fidx = [0,1,2,3,4])\n",
    "#meta_train.X = np.nan_to_num(meta_train.X, nan= -1)\n",
    "meta_test = Md_utils.md_concat([df_slots[i] for i in range(1)],fidx = [0,1,2,3,4])\n",
    "\n",
    "train_dataloader = meta_train.to_dataloader(train=True, batch_size=batch_size, num_workers=0)\n",
    "test_dataloader = meta_test.to_dataloader(train=False, batch_size=batch_size, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "n_features = meta_train.Z.shape[1]\n",
    "_, n_forecasters, horizon  = meta_train.B.shape\n",
    "window_x = meta_train.X.shape[2]\n",
    "\n",
    "n_known_features = len(meta_train.known_features)\n",
    "Use_z = True\n",
    "\n",
    "model = MetaNN.MetaComb(n_features= n_features, n_forecasters=n_forecasters,window_x=window_x,Use_z= Use_z,n_known_features= n_known_features, horizon=horizon).to(device)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=learning_rate)\n",
    "epochs = 10\n",
    "meta_learning = meta_learning_run(model,loss_fn, optimizer, train_dataloader,test_dataloader, device, epochs,True)\n",
    "loss, pred = meta_learning.forecast(test_dataloader)\n",
    "preds.append(np.expand_dims(pred.cpu().numpy(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metaselection example\n",
    "n_features = meta_train.Z.shape[1]\n",
    "_, n_forecasters, horizon  = meta_train.B.shape\n",
    "window_x = meta_train.X.shape[2]\n",
    "\n",
    "model = MetaNN.MetaSelection(n_features= n_features, n_forecasters=n_forecasters,window_x=window_x, Use_z= Use_z,n_known_features=n_known_features,horizon=horizon).to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=learning_rate)\n",
    "epochs = 20\n",
    "meta_learning = meta_learning_run(model,loss_fn, optimizer, train_dataloader,test_dataloader, device, epochs)\n",
    "loss, pred = meta_learning.forecast(test_dataloader)\n",
    "preds.append(np.expand_dims(pred.cpu().numpy(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## metalossn example\n",
    "n_features = meta_train.Z.shape[1]\n",
    "_, n_forecasters, horizon  = meta_train.B.shape\n",
    "window_x = meta_train.X.shape[2]\n",
    "\n",
    "model = MetaNN.MetaLoss(n_features= n_features, n_forecasters=n_forecasters,window_x=window_x, Use_z= Use_z,n_known_features= n_known_features,horizon=horizon).to(device)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer = torch.optim.Adam(model.parameters(),  lr=learning_rate)\n",
    "epochs = 30\n",
    "meta_learning = meta_learning_run(model,loss_fn, optimizer, train_dataloader,test_dataloader, device, epochs)\n",
    "loss, pred = meta_learning.forecast(test_dataloader)\n",
    "preds.append(np.expand_dims(pred.cpu().numpy(),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>sMAPE1</th>\n",
       "      <th>MAPE</th>\n",
       "      <th>RMSSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SimpleExpSmoothing</th>\n",
       "      <td>1.011806</td>\n",
       "      <td>4.680438</td>\n",
       "      <td>0.638231</td>\n",
       "      <td>0.675510</td>\n",
       "      <td>0.710221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ExponentialSmoothing</th>\n",
       "      <td>1.012420</td>\n",
       "      <td>4.685136</td>\n",
       "      <td>0.629418</td>\n",
       "      <td>0.676967</td>\n",
       "      <td>0.711432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm_direct</th>\n",
       "      <td>0.984526</td>\n",
       "      <td>3.708434</td>\n",
       "      <td>0.627456</td>\n",
       "      <td>0.710603</td>\n",
       "      <td>0.722578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lightgbm_mimo</th>\n",
       "      <td>0.984418</td>\n",
       "      <td>3.867940</td>\n",
       "      <td>0.629857</td>\n",
       "      <td>0.709617</td>\n",
       "      <td>0.711013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_mimo</th>\n",
       "      <td>1.024882</td>\n",
       "      <td>3.867784</td>\n",
       "      <td>0.623931</td>\n",
       "      <td>0.707739</td>\n",
       "      <td>0.757183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>meta_comb</th>\n",
       "      <td>0.975732</td>\n",
       "      <td>3.743444</td>\n",
       "      <td>0.627249</td>\n",
       "      <td>0.708764</td>\n",
       "      <td>0.705908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           MAE       MSE    sMAPE1      MAPE     RMSSE\n",
       "SimpleExpSmoothing    1.011806  4.680438  0.638231  0.675510  0.710221\n",
       "ExponentialSmoothing  1.012420  4.685136  0.629418  0.676967  0.711432\n",
       "lightgbm_direct       0.984526  3.708434  0.627456  0.710603  0.722578\n",
       "lightgbm_mimo         0.984418  3.867940  0.629857  0.709617  0.711013\n",
       "rf_mimo               1.024882  3.867784  0.623931  0.707739  0.757183\n",
       "meta_comb             0.975732  3.743444  0.627249  0.708764  0.705908"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(meta_test.evaluate(np.concatenate(preds,1),utils.metrics),index= meta_test.forecasters + ['meta_comb'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff6d4ce17eceee6fd1b58d61a6b43b34ae60da1a017957f957ae9fdd770bd0de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
